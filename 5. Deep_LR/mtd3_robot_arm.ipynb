{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于TD3算法的机械臂位置跟踪控制\n",
    "\n",
    "本笔记本实现了使用TD3(Twin Delayed Deep Deterministic Policy Gradient)算法控制机械臂进行位置跟踪任务。我们将使用MuJoCo作为物理仿真环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import mujoco\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# 尝试导入MuJoCo可视化器\n",
    "try:\n",
    "    import mujoco.viewer\n",
    "    MUJOCO_VIEWER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MUJOCO_VIEWER_AVAILABLE = False\n",
    "    print(\"警告: 无法导入mujoco.viewer，可视化功能将不可用\")\n",
    "\n",
    "# 添加Z-Score标准化类\n",
    "class RunningMeanStd:\n",
    "    \"\"\"\n",
    "    动态计算均值和标准差用于状态归一化\n",
    "    使用Welford在线算法计算均值和标准差\n",
    "    \"\"\"\n",
    "    def __init__(self, shape):\n",
    "        self.n = 0\n",
    "        self.mean = np.zeros(shape)\n",
    "        self.S = np.zeros(shape)\n",
    "        self.std = np.sqrt(self.S)\n",
    "\n",
    "    def update(self, x):\n",
    "        \"\"\"\n",
    "        更新均值和标准差\n",
    "        \n",
    "        参数:\n",
    "        - x: 新的状态数据\n",
    "        \"\"\"\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_S = np.sum((x - batch_mean)**2, axis=0)\n",
    "        batch_n = x.shape[0]\n",
    "        \n",
    "        # 更新总样本数\n",
    "        total_n = self.n + batch_n\n",
    "        \n",
    "        # 更新均值\n",
    "        delta = batch_mean - self.mean\n",
    "        new_mean = self.mean + delta * batch_n / total_n\n",
    "        \n",
    "        # 更新方差\n",
    "        delta2 = batch_mean - new_mean\n",
    "        new_S = self.S + batch_S + delta**2 * self.n * batch_n / total_n\n",
    "        \n",
    "        # 更新参数\n",
    "        self.n = total_n\n",
    "        self.mean = new_mean\n",
    "        self.S = new_S\n",
    "        self.std = np.sqrt(self.S / (self.n - 1)) if self.n > 1 else np.sqrt(self.S)\n",
    "        \n",
    "    def normalize(self, x):\n",
    "        \"\"\"\n",
    "        对输入数据进行归一化\n",
    "        \n",
    "        参数:\n",
    "        - x: 待归一化的状态数据\n",
    "        \n",
    "        返回:\n",
    "        - 归一化后的数据\n",
    "        \"\"\"\n",
    "        return (x - self.mean) / (self.std + 1e-8)  # 添加小常数防止除零错误\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查CUDA可用性\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义机械臂环境类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotArmEnv:\n",
    "    def __init__(self, model_path=\"robot_arm_mujoco.xml\", normalize_states=True):\n",
    "        # 加载MuJoCo模型\n",
    "        self.model = mujoco.MjModel.from_xml_path(model_path)\n",
    "        self.data = mujoco.MjData(self.model)\n",
    "        \n",
    "        # 获取关节数量\n",
    "        self.nu = self.model.nu\n",
    "        self.nq = self.model.nq\n",
    "        \n",
    "        # 设置动作空间\n",
    "        self.action_low = -20.0  # 扭矩限制\n",
    "        self.action_high = 20.0\n",
    "        \n",
    "        # 设置目标位置 (末端执行器)\n",
    "        self.target_pos = np.array([0.2, 0.2, 0.2])  # 初始目标位置\n",
    "        \n",
    "        # 初始化可视化器为None\n",
    "        self.viewer = None\n",
    "        \n",
    "        # 状态归一化参数\n",
    "        self.normalize_states = normalize_states\n",
    "        if self.normalize_states:\n",
    "            # 状态维度: 关节角度(6) + 关节速度(6) + 关节扭矩(6) + 末端位置(3) + 末端速度(3) + 目标位置(3)\n",
    "            state_dim = 6 + 6 + 6 + 3 + 3 + 3  # 27维状态\n",
    "            self.state_rms = RunningMeanStd(shape=(state_dim,))\n",
    "        \n",
    "        # 重置环境\n",
    "        self.reset()\n",
    "        \n",
    "    def render(self):\n",
    "        # 检查可视化器是否可用\n",
    "        if not MUJOCO_VIEWER_AVAILABLE:\n",
    "            return\n",
    "        \n",
    "        # 如果可视化器尚未创建，则创建一个\n",
    "        if self.viewer is None:\n",
    "            try:\n",
    "                # 尝试创建被动式可视化器\n",
    "                self.viewer = mujoco.viewer.launch_passive(self.model, self.data)\n",
    "                # 初始化目标可视化添加标记\n",
    "                self._target_viz_added = False\n",
    "            except Exception as e:\n",
    "                print(f\"无法启动可视化器: {e}\")\n",
    "                return\n",
    "        \n",
    "        # 如果可视化器已经创建，同步更新场景\n",
    "        if self.viewer:\n",
    "            try:\n",
    "                # 添加目标位置的可视化（绿色小球）\n",
    "                self._add_target_visualization()\n",
    "                self.viewer.sync()\n",
    "            except Exception as e:\n",
    "                print(f\"可视化器同步失败: {e}\")\n",
    "                self.viewer = None\n",
    "    \n",
    "    def _add_target_visualization(self):\n",
    "        \"\"\"\n",
    "        在目标位置添加一个绿色小球用于可视化\n",
    "        \"\"\"\n",
    "        if self.viewer and not getattr(self, '_target_viz_added', False):\n",
    "            # 获取用户场景对象\n",
    "            scn = self.viewer.user_scn\n",
    "            # 增加一个几何体（小球）\n",
    "            if scn.ngeom < scn.maxgeom:\n",
    "                # 获取新增几何体的引用\n",
    "                geom = scn.geoms[scn.ngeom]\n",
    "                scn.ngeom += 1\n",
    "                # 初始化几何体为球体\n",
    "                mujoco.mjv_initGeom(\n",
    "                    geom,\n",
    "                    mujoco.mjtGeom.mjGEOM_SPHERE,\n",
    "                    np.array([0.02, 0.02, 0.02]),  # 球体半径\n",
    "                    self.target_pos,  # 球体位置（目标位置）\n",
    "                    np.array([1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]),  # 单位矩阵\n",
    "                    np.array([0.0, 1.0, 0.0, 0.8])  # RGBA: 绿色，半透明\n",
    "                )\n",
    "                # 设置几何体类别为仅可视化，不参与物理碰撞\n",
    "                geom.category = mujoco.mjtCatBit.mjCAT_DECOR\n",
    "                # 标记已添加目标可视化\n",
    "                self._target_viz_added = True\n",
    "    \n",
    "    def reset(self):\n",
    "        # 重置MuJoCo数据\n",
    "        mujoco.mj_resetData(self.model, self.data)\n",
    "        \n",
    "        # 设置随机目标位置\n",
    "        self.target_pos = np.random.uniform(-0.3, 0.3, size=3)\n",
    "        \n",
    "        # 确保目标位置为合法值，x\\y不能在(-0.1, 0.1)范围内，z轴要大于0.05\n",
    "        if self.target_pos[0] >= 0:\n",
    "            self.target_pos[0] = max(self.target_pos[0], 0.1)\n",
    "        else:\n",
    "            self.target_pos[0] = min(self.target_pos[0], -0.1)\n",
    "\n",
    "        if self.target_pos[1] >= 0:\n",
    "            self.target_pos[1] = max(self.target_pos[1], 0.1)\n",
    "        else:\n",
    "            self.target_pos[1] = min(self.target_pos[1], -0.1)\n",
    "\n",
    "        self.target_pos[2] = max(self.target_pos[2], 0.05)\n",
    "\n",
    "        # 重置停留时间计数器\n",
    "        self.stay_time = 0.0\n",
    "        self.required_stay_time = 0.5  # 需要停留0.5秒\n",
    "        \n",
    "        # 获取模型的时间步长\n",
    "        self.time_step = self.model.opt.timestep\n",
    "\n",
    "        self.success_threshold = 0.001  # 1mm阈值\n",
    "        \n",
    "        # 重置步数计数器\n",
    "        self.step_count = 0\n",
    "        \n",
    "        # 重置目标可视化标记\n",
    "        if hasattr(self, '_target_viz_added'):\n",
    "            self._target_viz_added = False\n",
    "        \n",
    "        # 清理之前添加的可视化几何体\n",
    "        if self.viewer is not None:\n",
    "            # 重置场景中的几何体数量为0， effectively removing all added geometries\n",
    "            self.viewer.user_scn.ngeom = 0\n",
    "        \n",
    "        # 获取初始状态\n",
    "        state = self._get_state()\n",
    "        return state\n",
    "    \n",
    "    def _get_state(self):\n",
    "        # 获取关节角度\n",
    "        joint_angles = self.data.qpos[:self.nu].copy()\n",
    "        \n",
    "        # 获取关节速度\n",
    "        joint_velocities = self.data.qvel[:self.nu].copy()\n",
    "        \n",
    "        # 获取关节扭矩\n",
    "        joint_torques = self.data.qfrc_actuator[:self.nu].copy()\n",
    "        \n",
    "        # 获取末端执行器位置（通过名称查找）\n",
    "        ee_site_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_SITE, \"ee_site\")\n",
    "        \n",
    "        # 获取末端执行器位置\n",
    "        ee_pos = self.data.site_xpos[ee_site_id].copy()\n",
    "        \n",
    "        # 使用body速度获取末端执行器速度\n",
    "        ee_body_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, \"ee_link\")\n",
    "        ee_vel = self.data.cvel[ee_body_id][:3].copy()\n",
    "        \n",
    "        # 拼接状态向量\n",
    "        state = np.concatenate([\n",
    "            joint_angles,\n",
    "            joint_velocities,\n",
    "            joint_torques,\n",
    "            ee_pos,\n",
    "            ee_vel,\n",
    "            self.target_pos\n",
    "        ])\n",
    "        \n",
    "        # 如果启用状态归一化，则对状态进行归一化\n",
    "        if self.normalize_states:\n",
    "            state = self.state_rms.normalize(state)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 应用动作（扭矩）\n",
    "        action = np.clip(action, self.action_low, self.action_high)\n",
    "        self.data.ctrl[:self.nu] = action\n",
    "        \n",
    "        # 仿真一步\n",
    "        mujoco.mj_step(self.model, self.data)\n",
    "        \n",
    "        # 更新步数计数器\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # 获取新状态\n",
    "        state = self._get_state()\n",
    "        \n",
    "        # 计算奖励\n",
    "        ee_pos = self.data.site_xpos[0].copy()\n",
    "        distance = np.linalg.norm(ee_pos - self.target_pos)\n",
    "        \n",
    "        # 重新设计的奖励函数 - 包含密度奖励和分布奖励\n",
    "        \n",
    "        # 1. 密度奖励 - 鼓励持续向目标靠近\n",
    "        # 基础距离奖励，使用指数函数使接近目标时奖励增长更快\n",
    "        density_reward = 1.2 * (1 - np.exp(-2.0 * (0.6 - distance)))\n",
    "        \n",
    "        # 添加分段的密集距离奖励，鼓励精确控制\n",
    "        if distance < 0.001:\n",
    "            # 当距离非常近时(1mm内)，给予最大奖励以鼓励精确控制\n",
    "            density_reward += (0.001 - distance) * 300.0\n",
    "        elif distance < 0.005:\n",
    "            # 当距离很近时(5mm内)，给予较大奖励\n",
    "            density_reward += (0.005 - distance) * 100.0\n",
    "        elif distance < 0.01:\n",
    "            # 当距离较近时(1cm内)，给予适度奖励\n",
    "            density_reward += (0.01 - distance) * 50.0\n",
    "        elif distance < 0.05:\n",
    "            # 当距离中等时(5cm内)，给予适度奖励\n",
    "            density_reward += (0.05 - distance) * 10.0\n",
    "        elif distance < 0.1:\n",
    "            # 当距离稍远时(10cm内)，给予较小奖励\n",
    "            density_reward += (0.1 - distance) * 2.0\n",
    "        elif distance < 0.3:\n",
    "            # 当距离较远时(30cm内)，给予最小奖励\n",
    "            density_reward += (0.3 - distance) * 0.5\n",
    "            \n",
    "        # 2. 分布奖励 - 鼓励在整个轨迹中保持良好表现\n",
    "        distribution_reward = 0.0\n",
    "        \n",
    "        # 速度分布奖励 - 鼓励在整个过程中保持适中的速度\n",
    "        ee_body_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, \"ee_link\")\n",
    "        ee_vel = self.data.cvel[ee_body_id][:3].copy()\n",
    "        ee_speed = np.linalg.norm(ee_vel)\n",
    "        \n",
    "        # 鼓励保持适中的末端执行器速度 (0.01m/s - 0.1m/s)\n",
    "        if 0.01 <= ee_speed <= 0.1:\n",
    "            distribution_reward += 0.5\n",
    "        elif 0.005 <= ee_speed < 0.01:\n",
    "            distribution_reward += 0.3\n",
    "        elif 0.1 < ee_speed <= 0.2:\n",
    "            distribution_reward += 0.2\n",
    "        elif ee_speed < 0.005:\n",
    "            # 几乎静止时给予较小奖励，鼓励继续移动\n",
    "            distribution_reward += 0.1\n",
    "            \n",
    "        # 关节速度分布奖励 - 鼓励各关节速度分布均匀，避免某些关节过快\n",
    "        joint_velocities = self.data.qvel[:self.nu].copy()\n",
    "        joint_speeds = np.abs(joint_velocities)\n",
    "        avg_joint_speed = np.mean(joint_speeds)\n",
    "        \n",
    "        # 如果各关节速度相对均匀（标准差小），给予奖励\n",
    "        if len(joint_speeds) > 1:\n",
    "            speed_std = np.std(joint_speeds)\n",
    "            if speed_std < avg_joint_speed * 0.5:  # 相对标准差小于50%\n",
    "                distribution_reward += 0.3\n",
    "            elif speed_std < avg_joint_speed:  # 相对标准差小于100%\n",
    "                distribution_reward += 0.1\n",
    "                \n",
    "        # 关节加速度分布奖励 - 鼓励加速度变化平滑，减少抖动\n",
    "        if not hasattr(self, 'prev_joint_velocities'):\n",
    "            self.prev_joint_velocities = np.zeros(self.nu)\n",
    "            \n",
    "        joint_accelerations = (joint_velocities - self.prev_joint_velocities) / self.model.opt.timestep\n",
    "        self.prev_joint_velocities = joint_velocities.copy()\n",
    "        \n",
    "        # 惩罚过大的关节加速度（减少抖动）\n",
    "        joint_acc_magnitude = np.linalg.norm(joint_accelerations)\n",
    "        if joint_acc_magnitude < 10.0:  # 低加速度给予奖励\n",
    "            distribution_reward += 0.2\n",
    "        elif joint_acc_magnitude < 20.0:  # 中等加速度给予较小奖励\n",
    "            distribution_reward += 0.1\n",
    "            \n",
    "        # 3. 平稳性奖励 - 鼓励运动平稳、减少抖动\n",
    "        smoothness_reward = 0.0\n",
    "        \n",
    "        # 动作变化惩罚 - 鼓励动作变化平滑\n",
    "        if not hasattr(self, 'prev_action'):\n",
    "            self.prev_action = np.zeros(self.nu)\n",
    "            \n",
    "        action_change = np.linalg.norm(action - self.prev_action)\n",
    "        self.prev_action = action.copy()\n",
    "        \n",
    "        # 惩罚过大的动作变化\n",
    "        if action_change < 2.0:  # 小动作变化给予奖励\n",
    "            smoothness_reward += 0.3\n",
    "        elif action_change < 5.0:  # 中等动作变化给予较小奖励\n",
    "            smoothness_reward += 0.1\n",
    "            \n",
    "        # 关节速度惩罚 - 惩罚过高的关节速度，鼓励平稳运动\n",
    "        joint_speed_penalty = -0.005 * np.sum(np.square(joint_velocities))\n",
    "        \n",
    "        # 4. 其他奖励和惩罚项\n",
    "        # 动作惩罚（鼓励使用较小的力）\n",
    "        action_penalty = -0.001 * np.sum(np.square(action))\n",
    "        \n",
    "        # 碰撞惩罚 - 检查是否有接触\n",
    "        collision_penalty = 0.0\n",
    "        collision_detected = False\n",
    "        for i in range(self.data.ncon):\n",
    "            contact = self.data.contact[i]\n",
    "            # 如果检测到碰撞，给予惩罚\n",
    "            collision_penalty -= 100.0\n",
    "            collision_detected = True\n",
    "        \n",
    "        # 高度惩罚 - 防止机械臂触碰地面\n",
    "        height_penalty = 0.0\n",
    "        if ee_pos[2] < 0.05:  # 如果末端执行器高度低于5cm\n",
    "            height_penalty = -5.0 * (0.05 - ee_pos[2])\n",
    "        \n",
    "        # 垂直方向奖励 - 鼓励机械臂保持在合理高度\n",
    "        vertical_reward = 0.0\n",
    "        if 0.05 <= ee_pos[2] <= 0.4:  # 如果末端执行器在合理高度范围内\n",
    "            vertical_reward = 0.1\n",
    "            \n",
    "        # 仿真步数惩罚 - 鼓励尽快完成任务\n",
    "        step_penalty = -0.001 * self.step_count\n",
    "        \n",
    "        # 计算总奖励\n",
    "        reward = (density_reward + \n",
    "                 distribution_reward + \n",
    "                 smoothness_reward + \n",
    "                 joint_speed_penalty + \n",
    "                 action_penalty + \n",
    "                 collision_penalty + \n",
    "                 height_penalty + \n",
    "                 vertical_reward + \n",
    "                 step_penalty)\n",
    "        \n",
    "        # 检查是否完成\n",
    "        distance_to_target = np.linalg.norm(ee_pos - self.target_pos)\n",
    "        \n",
    "        done = False\n",
    "        success = False\n",
    "        # 新的成功条件：只要碰到目标位置就算成功（距离小于1mm）\n",
    "        if distance_to_target < self.success_threshold:\n",
    "            done = True\n",
    "            success = True\n",
    "            \n",
    "            # 成功奖励\n",
    "            success_reward = 200.0\n",
    "            \n",
    "            # 速度奖励：成功时速度越慢奖励越高\n",
    "            if ee_speed < 0.01:\n",
    "                speed_reward_on_success = 100.0\n",
    "            elif ee_speed < 0.05:\n",
    "                speed_reward_on_success = 50.0\n",
    "            elif ee_speed < 0.1:\n",
    "                speed_reward_on_success = 20.0\n",
    "            else:\n",
    "                speed_reward_on_success = 0.0\n",
    "                \n",
    "            reward += success_reward + speed_reward_on_success\n",
    "        \n",
    "        # 如果发生碰撞，也结束episode\n",
    "        if collision_detected:\n",
    "            done = True\n",
    "        \n",
    "        # 返回奖励成分信息，用于分析和可视化\n",
    "        reward_info = {\n",
    "            'density_reward': density_reward,\n",
    "            'distribution_reward': distribution_reward,\n",
    "            'smoothness_reward': smoothness_reward,\n",
    "            'joint_speed_penalty': joint_speed_penalty,\n",
    "            'action_penalty': action_penalty,\n",
    "            'collision_penalty': collision_penalty,\n",
    "            'height_penalty': height_penalty,\n",
    "            'vertical_reward': vertical_reward,\n",
    "            'step_penalty': step_penalty\n",
    "        }\n",
    "            \n",
    "        return state, reward, done, {'reward_info': reward_info}\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        关闭可视化器\n",
    "        \"\"\"\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义Actor和Critic网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, action_high=1.0):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # 确保action_high是一个序列，如果不是则转换为序列\n",
    "        if isinstance(action_high, (int, float)):\n",
    "            self.action_scale = torch.FloatTensor([action_high] * action_dim)\n",
    "        else:\n",
    "            self.action_scale = torch.FloatTensor(action_high)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        action = self.actor(state)\n",
    "        # 确保action_scale与state在同一设备上\n",
    "        if self.action_scale.device != state.device:\n",
    "            self.action_scale = self.action_scale.to(state.device)\n",
    "        action = action * self.action_scale\n",
    "        return action\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        # 第一个Q网络\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # 第二个Q网络\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "        q1 = self.q1(sa)\n",
    "        q2 = self.q2(sa)\n",
    "        return q1, q2\n",
    "    \n",
    "    def Q1(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "        q1 = self.q1(sa)\n",
    "        return q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义TD3智能体和经验回放缓冲区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3Agent:\n",
    "    def __init__(self, state_dim, action_dim, action_high=20.0, \n",
    "                 lr_actor=3e-4, lr_critic=3e-4, gamma=0.99, tau=0.005, \n",
    "                 policy_noise=0.2, noise_clip=0.5, policy_freq=2, exploration_noise_std=0.1):\n",
    "        \"\"\"\n",
    "        TD3算法智能体初始化\n",
    "        \n",
    "        超参数说明:\n",
    "        - lr_actor (float): Actor网络学习率\n",
    "          作用: 控制Actor网络参数更新的步长\n",
    "          调整建议: 通常在[1e-4, 1e-3]范围内，较小值训练稳定但收敛慢，较大值收敛快但可能不稳定\n",
    "          \n",
    "        - lr_critic (float): Critic网络学习率\n",
    "          作用: 控制Critic网络参数更新的步长\n",
    "          调整建议: 通常与lr_actor相同或稍大，一般在[1e-4, 1e-3]范围内\n",
    "          \n",
    "        - gamma (float): 折扣因子\n",
    "          作用: 衡量未来奖励的重要性，值越接近1表示越重视长期奖励\n",
    "          调整建议: 通常设置为0.99左右，对于更注重即时奖励的任务可适当降低\n",
    "          \n",
    "        - tau (float): 目标网络软更新系数\n",
    "          作用: 控制目标网络向主网络更新的速度，值越小更新越慢\n",
    "          调整建议: 通常设置为0.001-0.01，过大会导致训练不稳定，过小会导致收敛缓慢\n",
    "          \n",
    "        - policy_noise (float): 策略噪声标准差\n",
    "          作用: 在目标策略中添加噪声以平滑Q函数，减少过估计\n",
    "          调整建议: 通常设置为动作空间范围的10-20%，需根据具体动作范围调整\n",
    "          \n",
    "        - noise_clip (float): 噪声裁剪范围\n",
    "          作用: 限制添加到目标策略中的噪声范围，保持目标策略的合理性\n",
    "          调整建议: 通常是policy_noise的2-3倍，防止噪声过大\n",
    "          \n",
    "        - policy_freq (int): 策略更新频率\n",
    "          作用: 控制Actor网络更新频率，相对于Critic网络更新的倍数\n",
    "          调整建议: 通常设置为2，表示Critic更新2次后Actor更新1次，增加训练稳定性\n",
    "          \n",
    "        - exploration_noise_std (float): 探索噪声标准差\n",
    "          作用: 在动作选择时添加噪声以促进探索\n",
    "          调整建议: 根据动作空间范围调整，通常为动作范围的5-15%，过大会导致行动不稳定，过小探索不足\n",
    "        \"\"\"\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma  # 折扣因子，控制未来奖励的重要性\n",
    "        self.tau = tau      # 目标网络软更新系数\n",
    "        self.policy_noise = policy_noise  # 目标策略平滑噪声标准差\n",
    "        self.noise_clip = noise_clip      # 目标策略平滑噪声裁剪范围\n",
    "        self.policy_freq = policy_freq    # 策略更新频率（相对于Critic更新）\n",
    "        self.action_high = action_high    # 动作上限\n",
    "        self.exploration_noise_std = exploration_noise_std  # 探索噪声标准差\n",
    "        \n",
    "        self.total_it = 0\n",
    "        \n",
    "        # 创建网络\n",
    "        self.actor = Actor(state_dim, action_dim, action_high=action_high).to(self.device)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        \n",
    "        self.critic = Critic(state_dim, action_dim).to(self.device)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "    \n",
    "    def select_action(self, state, noise_std=None):\n",
    "        \"\"\"\n",
    "        选择动作\n",
    "        \n",
    "        参数:\n",
    "        - state: 当前状态\n",
    "        - noise_std (float): 探索噪声标准差\n",
    "          作用: 控制添加到动作中的噪声强度，用于探索\n",
    "          调整建议: 训练时使用exploration_noise_std，测试时设为0或很小的值\n",
    "        \"\"\"\n",
    "        # 如果没有指定noise_std，则使用初始化时设置的默认值\n",
    "        # exploration_noise_std: 探索噪声标准差，控制探索强度\n",
    "        if noise_std is None:\n",
    "            noise_std = self.exploration_noise_std\n",
    "            \n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        \n",
    "        # 添加探索噪声\n",
    "        if noise_std > 0:\n",
    "            noise = np.random.normal(0, noise_std, size=self.action_dim)\n",
    "            action = action + noise\n",
    "        \n",
    "        # 限制动作范围\n",
    "        action = np.clip(action, -self.action_high, self.action_high)\n",
    "        return action\n",
    "    \n",
    "    def train(self, replay_buffer, batch_size=256):\n",
    "        \"\"\"\n",
    "        训练智能体\n",
    "        \n",
    "        参数:\n",
    "        - replay_buffer: 经验回放缓冲区\n",
    "        - batch_size (int): 批处理大小\n",
    "          作用: 每次训练使用的样本数量\n",
    "          调整建议: 通常设置为128-512，较大的batch_size训练更稳定但需要更多计算资源\n",
    "        \"\"\"\n",
    "        self.total_it += 1\n",
    "        \n",
    "        # 从经验回放缓冲区采样\n",
    "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 添加目标策略平滑\n",
    "            # policy_noise: 控制目标策略平滑的噪声强度\n",
    "            # noise_clip: 限制噪声范围，防止噪声过大\n",
    "            noise = torch.randn_like(action) * self.policy_noise\n",
    "            noise = noise.clamp(-self.noise_clip, self.noise_clip)\n",
    "            \n",
    "            next_action = self.actor_target(next_state) + noise\n",
    "            next_action = next_action.clamp(-self.action_high, self.action_high)\n",
    "            \n",
    "            # 计算目标Q值\n",
    "            # gamma: 折扣因子，平衡即时奖励和未来奖励\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + not_done * self.gamma * target_Q\n",
    "        \n",
    "        # 获取当前Q值\n",
    "        current_Q1, current_Q2 = self.critic(state, action)\n",
    "        \n",
    "        # 计算critic损失\n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "        \n",
    "        # 优化critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # 延迟策略更新\n",
    "        # policy_freq: 控制策略更新频率，减少策略更新次数提高稳定性\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "            \n",
    "            # 计算actor损失\n",
    "            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "            \n",
    "            # 优化actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            \n",
    "            # 更新目标网络\n",
    "            # tau: 目标网络软更新系数，控制更新速度\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                \n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                \n",
    "        return critic_loss.item()\n",
    "    \n",
    "    def save(self, filename, state_rms=None):\n",
    "        \"\"\"\n",
    "        保存模型参数\n",
    "        \"\"\"\n",
    "        save_dict = {\n",
    "            'actor_state_dict': self.actor.state_dict(),\n",
    "            'critic_state_dict': self.critic.state_dict(),\n",
    "            'actor_target_state_dict': self.actor_target.state_dict(),\n",
    "            'critic_target_state_dict': self.critic_target.state_dict(),\n",
    "            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\n",
    "            'total_it': self.total_it\n",
    "        }\n",
    "        \n",
    "        # 如果提供了状态归一化参数，则一并保存\n",
    "        if state_rms is not None:\n",
    "            save_dict['state_rms_mean'] = state_rms.mean\n",
    "            save_dict['state_rms_std'] = state_rms.std\n",
    "            save_dict['state_rms_n'] = state_rms.n\n",
    "            save_dict['state_rms_S'] = state_rms.S\n",
    "        \n",
    "        torch.save(save_dict, filename)\n",
    "        print(f\"模型已保存至 {filename}\")\n",
    "    \n",
    "    def load(self, filename, env=None):\n",
    "        \"\"\"\n",
    "        加载模型参数\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(filename, map_location=self.device)\n",
    "        self.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "        self.actor_target.load_state_dict(checkpoint['actor_target_state_dict'])\n",
    "        self.critic_target.load_state_dict(checkpoint['critic_target_state_dict'])\n",
    "        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
    "        self.total_it = checkpoint['total_it']\n",
    "        \n",
    "        # 如果环境对象和状态归一化参数都存在，则恢复状态归一化参数\n",
    "        if env is not None and hasattr(env, 'state_rms') and 'state_rms_mean' in checkpoint:\n",
    "            env.state_rms.mean = checkpoint['state_rms_mean']\n",
    "            env.state_rms.std = checkpoint['state_rms_std']\n",
    "            env.state_rms.n = checkpoint['state_rms_n']\n",
    "            env.state_rms.S = checkpoint['state_rms_S']\n",
    "            print(\"状态归一化参数已恢复\")\n",
    "        \n",
    "        print(f\"模型已从 {filename} 加载\")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "        \"\"\"\n",
    "        经验回放缓冲区\n",
    "        \n",
    "        超参数说明:\n",
    "        - max_size (int): 经验回放缓冲区最大容量\n",
    "          作用: 控制存储经验的数量，影响学习效率和内存占用\n",
    "          调整建议: 通常设置为1e6左右，较大的缓冲区可以存储更多样化的经验，但占用更多内存\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        self.state = np.zeros((max_size, state_dim))\n",
    "        self.action = np.zeros((max_size, action_dim))\n",
    "        self.next_state = np.zeros((max_size, state_dim))\n",
    "        self.reward = np.zeros((max_size, 1))\n",
    "        self.not_done = np.zeros((max_size, 1))\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        \"\"\"\n",
    "        向经验回放缓冲区添加经验\n",
    "        \"\"\"\n",
    "        self.state[self.ptr] = state\n",
    "        self.action[self.ptr] = action\n",
    "        self.next_state[self.ptr] = next_state\n",
    "        self.reward[self.ptr] = reward\n",
    "        self.not_done[self.ptr] = 1. - done\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        从经验回放缓冲区中采样一批经验\n",
    "        \n",
    "        参数:\n",
    "        - batch_size (int): 采样批次大小\n",
    "          作用: 控制每次训练使用的样本数量\n",
    "          调整建议: 通常设置为128-512，需要在训练稳定性和计算效率之间平衡\n",
    "        \"\"\"\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(self.state[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.action[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练和测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, replay_buffer, num_episodes=1000, max_steps=5000, start_timesteps=50000, \n",
    "                save_freq=50, model_path=\"td3_robot_arm.pth\", render=False, normalize_states=True):\n",
    "    \"\"\"\n",
    "    训练智能体\n",
    "    \n",
    "    超参数说明:\n",
    "    - num_episodes (int): 训练episode总数\n",
    "      作用: 控制训练的总时长\n",
    "      调整建议: 根据任务复杂度调整，简单任务可设置较小值，复杂任务需要更多episode\n",
    "      \n",
    "    - max_steps (int): 每个episode的最大步数\n",
    "      作用: 防止episode无限进行下去\n",
    "      调整建议: 根据任务特点设置，确保智能体有足够时间完成任务\n",
    "      \n",
    "    - start_timesteps (int): 开始学习前的随机探索步数\n",
    "      作用: 收集初始经验，填充经验回放缓冲区\n",
    "      调整建议: 通常设置为几千到几万步，确保缓冲区有足够多样化的经验\n",
    "      \n",
    "    - save_freq (int): 模型保存频率\n",
    "      作用: 控制模型保存间隔，防止训练中断丢失进度\n",
    "      调整建议: 根据训练时长设置，通常每隔几十到几百个episode保存一次\n",
    "    \n",
    "    - normalize_states (bool): 是否对状态进行归一化\n",
    "      作用: 控制是否启用状态归一化功能\n",
    "      调整建议: 默认启用，有助于提高训练稳定性\n",
    "    \"\"\"\n",
    "    # 如果启用状态归一化，先收集状态统计数据\n",
    "    if normalize_states:\n",
    "        print(\"收集状态统计数据用于归一化...\")\n",
    "        random_states = collect_random_states(env, num_episodes=2000)\n",
    "        env.state_rms.update(random_states)\n",
    "        print(f\"状态均值: {env.state_rms.mean}\")\n",
    "        print(f\"状态标准差: {env.state_rms.std}\")\n",
    "    \n",
    "    # 记录训练过程\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    episode_times = []\n",
    "    \n",
    "    # 记录奖励成分的变化\n",
    "    reward_components_history = {\n",
    "        'density_reward': [],\n",
    "        'distribution_reward': [],\n",
    "        'smoothness_reward': [],\n",
    "        'joint_speed_penalty': [],\n",
    "        'action_penalty': [],\n",
    "        'collision_penalty': [],\n",
    "        'height_penalty': [],\n",
    "        'vertical_reward': [],\n",
    "        'step_penalty': []\n",
    "    }\n",
    "    \n",
    "    # 用于存储最近100个episode的奖励\n",
    "    recent_rewards = deque(maxlen=100)\n",
    "    \n",
    "    total_timesteps = 0\n",
    "    \n",
    "    # 创建模型保存目录\n",
    "    os.makedirs(os.path.dirname(model_path) if os.path.dirname(model_path) else '.', exist_ok=True)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # 重置环境\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        \n",
    "        # 记录episode开始时间\n",
    "        episode_start_time = time.time()\n",
    "        \n",
    "        # 为当前episode初始化奖励成分记录\n",
    "        episode_reward_components = {key: 0 for key in reward_components_history.keys()}\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # 选择动作\n",
    "            if total_timesteps < start_timesteps:\n",
    "                # 在开始阶段使用随机动作进行探索\n",
    "                action = np.random.uniform(-env.action_high, env.action_high, size=env.nu)\n",
    "            else:\n",
    "                action = agent.select_action(state)\n",
    "            \n",
    "            # 执行动作\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # 可视化训练过程\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(0.01)  # 控制渲染速度\n",
    "            \n",
    "            # 存储经验\n",
    "            replay_buffer.add(state, action, next_state, reward, done)\n",
    "            \n",
    "            # 训练智能体\n",
    "            if total_timesteps >= start_timesteps:\n",
    "                loss = agent.train(replay_buffer)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            total_timesteps += 1\n",
    "            \n",
    "            # 累加奖励成分\n",
    "            if 'reward_info' in info:\n",
    "                for key in episode_reward_components.keys():\n",
    "                    if key in info['reward_info']:\n",
    "                        episode_reward_components[key] += info['reward_info'][key]\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # 记录统计信息\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(step + 1)\n",
    "        recent_rewards.append(episode_reward)\n",
    "        \n",
    "        # 记录episode的奖励成分\n",
    "        for key in reward_components_history.keys():\n",
    "            reward_components_history[key].append(episode_reward_components[key])\n",
    "        \n",
    "        # 计算episode耗时\n",
    "        episode_time = time.time() - episode_start_time\n",
    "        episode_times.append(episode_time)\n",
    "        \n",
    "        # 打印进度\n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = np.mean(recent_rewards)\n",
    "            avg_step = np.mean(episode_lengths)\n",
    "            avg_episode_time = np.mean(episode_times)\n",
    "            print(f\"Episode {episode}, 平均奖励: {avg_reward:.2f}, 平均Episode长度: {avg_step:.2f}, 平均耗时: {avg_episode_time:.2f}秒, 总步数: {total_timesteps}, 运行时间: {time.time() - start_time:.2f}秒\")\n",
    "            recent_rewards.clear()\n",
    "            episode_lengths.clear()\n",
    "            episode_times.clear()\n",
    "\n",
    "        # 定期保存模型，同时保存状态归一化参数\n",
    "        if episode % save_freq == 0:\n",
    "            if normalize_states:\n",
    "                agent.save(f\"{model_path}_episode_{episode}.pth\", state_rms=env.state_rms)\n",
    "            else:\n",
    "                agent.save(f\"{model_path}_episode_{episode}.pth\")\n",
    "    \n",
    "    # 保存最终模型\n",
    "    if normalize_states:\n",
    "        agent.save(f\"{model_path}_final.pth\", state_rms=env.state_rms)\n",
    "    else:\n",
    "        agent.save(f\"{model_path}_final.pth\")\n",
    "    \n",
    "    return episode_rewards, episode_lengths, reward_components_history\n",
    "\n",
    "def resume_training(env, agent, replay_buffer, resume_episode, num_episodes=1000, max_steps=500, \n",
    "                    start_timesteps=10000, save_freq=50, model_path=\"td3_robot_arm.pth\", render=False):\n",
    "    \"\"\"\n",
    "    从指定episode继续训练\n",
    "    \"\"\"\n",
    "    # 加载模型\n",
    "    model_file = f\"{model_path}_episode_{resume_episode}.pth\"\n",
    "    if os.path.exists(model_file):\n",
    "        agent.load(model_file, env=env)\n",
    "        print(f\"从episode {resume_episode} 继续训练\")\n",
    "    else:\n",
    "        print(f\"未找到模型文件 {model_file}，从头开始训练\")\n",
    "        resume_episode = 0\n",
    "    \n",
    "    # 调用常规训练函数\n",
    "    return train_agent(env, agent, replay_buffer, num_episodes, max_steps, start_timesteps, save_freq, model_path, render)\n",
    "\n",
    "def collect_random_states(env, num_episodes=100):\n",
    "    \"\"\"\n",
    "    使用随机策略收集状态数据，用于计算状态归一化参数\n",
    "    \n",
    "    参数:\n",
    "    - env: 环境实例\n",
    "    - num_episodes: 收集数据的回合数\n",
    "    \n",
    "    返回:\n",
    "    - 所有收集到的状态数据\n",
    "    \"\"\"\n",
    "    all_states = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        all_states.append(state)\n",
    "        \n",
    "        # 使用随机动作探索环境\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.random.uniform(-env.action_high, env.action_high, size=env.nu)\n",
    "            state, _, done, _ = env.step(action)\n",
    "            all_states.append(state)\n",
    "            \n",
    "    return np.array(all_states)\n",
    "\n",
    "# 测试训练好的智能体\n",
    "def test_agent(env, agent, num_episodes=10, render=True):\n",
    "    # 记录每个episode的奖励成分\n",
    "    all_reward_components = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # 初始化当前episode的奖励成分记录\n",
    "        episode_reward_components = {\n",
    "            'density_reward': 0,\n",
    "            'distribution_reward': 0,\n",
    "            'smoothness_reward': 0,\n",
    "            'joint_speed_penalty': 0,\n",
    "            'action_penalty': 0,\n",
    "            'collision_penalty': 0,\n",
    "            'height_penalty': 0,\n",
    "            'vertical_reward': 0,\n",
    "            'step_penalty': 0\n",
    "        }\n",
    "        \n",
    "        for step in range(200):\n",
    "            action = agent.select_action(state, noise_std=0)  # 测试时不添加噪声\n",
    "            state, reward, done, info = env.step(action)\n",
    "            if render:\n",
    "                env.render()  # 在测试时进行可视化\n",
    "                time.sleep(0.01)  # 控制渲染速度\n",
    "            total_reward += reward\n",
    "            \n",
    "            # 累加奖励成分\n",
    "            if 'reward_info' in info:\n",
    "                for key in episode_reward_components.keys():\n",
    "                    if key in info['reward_info']:\n",
    "                        episode_reward_components[key] += info['reward_info'][key]\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        print(f\"测试Episode {episode + 1}: 总奖励 = {total_reward:.2f}, 步数 = {step + 1}\")\n",
    "        \n",
    "        # 保存当前episode的奖励成分\n",
    "        all_reward_components.append(episode_reward_components)\n",
    "    \n",
    "    # 关闭可视化器\n",
    "    env.close()\n",
    "    \n",
    "    # 绘制奖励成分曲线\n",
    "    if all_reward_components:\n",
    "        # 准备数据\n",
    "        components_keys = list(all_reward_components[0].keys())\n",
    "        components_data = {key: [ep[key] for ep in all_reward_components] for key in components_keys}\n",
    "        \n",
    "        # 绘制各种奖励成分（分别绘制）\n",
    "        components_to_plot = [\n",
    "            ('density_reward', 'Density Reward'),\n",
    "            ('distribution_reward', 'Distribution Reward'),\n",
    "            ('smoothness_reward', 'Smoothness Reward'),\n",
    "            ('joint_speed_penalty', 'Joint Speed Penalty'),\n",
    "            ('action_penalty', 'Action Penalty'),\n",
    "            ('collision_penalty', 'Collision Penalty')\n",
    "        ]\n",
    "        \n",
    "        # 为每个奖励成分创建单独的图表\n",
    "        for comp_key, comp_name in components_to_plot:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(components_data[comp_key], marker='o')\n",
    "            plt.title(f'{comp_name} over Episodes')\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel(comp_name)\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # 绘制奖励成分堆叠图\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # 准备堆叠数据\n",
    "        stack_data = [components_data[key] for key, _ in components_to_plot]\n",
    "        stack_labels = [name for _, name in components_to_plot]\n",
    "        \n",
    "        # 绘制堆叠区域图\n",
    "        plt.stackplot(range(len(all_reward_components)), stack_data, labels=stack_labels)\n",
    "        plt.title('Reward Components Composition')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward Value')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建环境和智能体\n",
    "env = RobotArmEnv(normalize_states=True)\n",
    "\n",
    "# 状态维度: 关节角度(6) + 关节速度(6) + 关节扭矩(6) + 末端位置(3) + 末端速度(3) + 目标位置(3)\n",
    "state_dim = 6 + 6 + 6 + 3 + 3 + 3  # 27维状态\n",
    "action_dim = env.nu  # 6维动作（扭矩）\n",
    "\n",
    "print(f\"状态维度: {state_dim}\")\n",
    "print(f\"动作维度: {action_dim}\")\n",
    "\n",
    "# 超参数设置\n",
    "# 学习率设置\n",
    "lr_actor=3e-4          # Actor网络学习率，控制策略网络参数更新步长\n",
    "                       # 值域范围：[1e-4, 1e-3]，较小值训练稳定但收敛慢，较大值收敛快但可能不稳定\n",
    "                       # 推荐值：3e-4是深度强化学习中的常用值，平衡训练稳定性和收敛速度\n",
    "lr_critic=3e-4         # Critic网络学习率，控制价值网络参数更新步长\n",
    "                       # 值域范围：[1e-4, 1e-3]，通常与lr_actor相同或稍大\n",
    "                       # 推荐值：3e-4，与Actor网络保持一致以维持训练平衡\n",
    "\n",
    "# 环境和奖励相关参数\n",
    "gamma=0.99             # 折扣因子，衡量未来奖励的重要性\n",
    "                       # 值域范围：[0.9, 0.999]，值越接近1表示越重视长期奖励\n",
    "                       # 推荐值：0.99，适用于大多数连续控制任务\n",
    "\n",
    "# 目标网络更新参数\n",
    "tau=0.005              # 目标网络软更新系数，控制目标网络向主网络更新的速度\n",
    "                       # 值域范围：[0.001, 0.05]，过大会导致训练不稳定，过小会导致收敛缓慢\n",
    "                       # 推荐值：0.005，在稳定性和收敛速度之间良好平衡\n",
    "\n",
    "# 策略噪声参数\n",
    "policy_noise=0.2       # 策略噪声标准差，在目标策略中添加噪声以平滑Q函数，减少过估计\n",
    "                       # 值域范围：通常设置为动作空间范围的10-20%\n",
    "                       # 推荐值：0.2，假设动作范围为[-1, 1]，需要根据具体动作范围调整\n",
    "noise_clip=1.0         # 噪声裁剪范围，限制添加到目标策略中的噪声范围，保持目标策略的合理性\n",
    "                       # 值域范围：通常是policy_noise的2-5倍\n",
    "                       # 推荐值：1.0，约为policy_noise的5倍\n",
    "\n",
    "# 策略更新频率\n",
    "policy_freq=2          # 策略更新频率，控制Actor网络更新频率，相对于Critic网络更新的倍数\n",
    "                       # 值域范围：[1, 10]，值越大更新越慢\n",
    "                       # 推荐值：2，表示Critic更新2次后Actor更新1次，增加训练稳定性\n",
    "\n",
    "# 探索噪声参数\n",
    "exploration_noise_std=0.12  # 探索噪声标准差，在动作选择时添加噪声以促进探索\n",
    "                           # 值域范围：根据动作空间范围调整，通常为动作范围的5-15%\n",
    "                           # 推荐值：0.1，对于机械臂控制任务提供良好探索与稳定性的平衡\n",
    "                           # 调整建议：如果策略收敛慢或陷入局部最优可适当增加到0.15-0.2\n",
    "                           #          如果动作抖动过大或无法精确控制可降低到0.05-0.1\n",
    "\n",
    "# 训练控制参数\n",
    "num_episodes = 20000   # 训练episode数，控制训练的总时长\n",
    "                       # 值域范围：根据任务复杂度调整，简单任务可设置较小值，复杂任务需要更多episode\n",
    "                       # 推荐值：1000，适用于中等复杂度的机械臂控制任务\n",
    "save_freq = 100        # 模型保存频率，控制模型保存间隔，防止训练中断丢失进度\n",
    "                       # 值域范围：根据训练时长设置，通常每隔几十到几百个episode保存一次\n",
    "                       # 推荐值：100，平衡了模型保存开销和进度保护需求\n",
    "\n",
    "TEST_MODE = True\n",
    "test_model_path = \"models/td3_robot_arm_episode_800.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演示如何加载已保存的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE:\n",
    "    print(\"演示模型加载:\")\n",
    "    # 创建一个新的智能体实例\n",
    "    new_agent = TD3Agent(state_dim, action_dim, action_high=env.action_high)\n",
    "\n",
    "    # 加载保存的模型（这里加载最终模型，实际使用时可以加载任何保存的模型）\n",
    "    new_agent.load(test_model_path, env=env)\n",
    "    print(f\"模型加载成功:{test_model_path}\")\n",
    "\n",
    "    print(\"测试加载的模型:\")\n",
    "    # 测试时默认开启可视化，可以通过设置render=False关闭可视化\n",
    "    test_agent(env, new_agent, render=True)\n",
    "\n",
    "    # 关闭环境的可视化器\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建环境和智能体并开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TEST_MODE:\n",
    "    # 创建TD3智能体\n",
    "    agent = TD3Agent(state_dim, action_dim, action_high=env.action_high, \n",
    "                    lr_actor=lr_actor, lr_critic=lr_critic, gamma=gamma, tau=tau, \n",
    "                    policy_noise=policy_noise, noise_clip=noise_clip, \n",
    "                    policy_freq=policy_freq, exploration_noise_std=exploration_noise_std)\n",
    "    replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "    # 开始训练\n",
    "    print(\"开始训练TD3智能体...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 可以通过设置render=True来可视化训练过程，设置render=False则不进行可视化\n",
    "    episode_rewards, episode_lengths, reward_components_history = train_agent(env, agent, replay_buffer, num_episodes=num_episodes, model_path=\"models/td3_robot_arm\", render=False, save_freq=save_freq, normalize_states=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"训练完成，耗时: {end_time - start_time:.2f}秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘制训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TEST_MODE:\n",
    "    # 绘制训练结果\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.title('Episode Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Rewards')\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(episode_lengths)\n",
    "    plt.title('Episode Steps')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 绘制各种奖励成分（分别绘制）\n",
    "    components_to_plot = [\n",
    "        ('density_reward', 'Density Reward'),\n",
    "        ('distribution_reward', 'Distribution Reward'),\n",
    "        ('smoothness_reward', 'Smoothness Reward'),\n",
    "        ('joint_speed_penalty', 'Joint Speed Penalty'),\n",
    "        ('action_penalty', 'Action Penalty'),\n",
    "        ('collision_penalty', 'Collision Penalty')\n",
    "    ]\n",
    "\n",
    "    # 为每个奖励成分创建单独的图表\n",
    "    for comp_key, comp_name in components_to_plot:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(reward_components_history[comp_key])\n",
    "        plt.title(f'{comp_name} over Episodes')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel(comp_name)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 绘制奖励成分堆叠图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # 准备堆叠数据\n",
    "    components_data = [reward_components_history[key] for key in [\n",
    "        'density_reward', 'distribution_reward', 'smoothness_reward',\n",
    "        'joint_speed_penalty', 'action_penalty', 'collision_penalty'\n",
    "    ]]\n",
    "\n",
    "    components_labels = [\n",
    "        'Density Reward', 'Distribution Reward', 'Smoothness Reward',\n",
    "        'Joint Speed Penalty', 'Action Penalty', 'Collision Penalty'\n",
    "    ]\n",
    "\n",
    "    # 绘制堆叠区域图\n",
    "    plt.stackplot(range(len(episode_rewards)), components_data, labels=components_labels)\n",
    "    plt.title('Reward Components Composition')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试训练好的智能体\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TEST_MODE:\n",
    "    print(\"测试训练好的智能体:\")\n",
    "    # 测试时默认开启可视化，可以通过设置render=False关闭可视化\n",
    "    test_agent(env, agent, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
